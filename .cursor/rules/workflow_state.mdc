---
description: Workflow state template for AI agent autonomous task execution and state management
globs: 
alwaysApply: false
---
# Workflow State (STM) - 2025-01-23 02:09:00

## 0. Current Overall Goal
- Autonomously process tasks using available task management systems (preferably Task-Master), implement solutions following project standards, validate implementation, and commit changes.

## 1. State
- **Phase:** `COMMITTING`
- **Status:** `COMPLETED_ITERATION_SUCCESS`

## 2. Current Task
- **Task ID/Raw Output:** Task ID: 1 - "Database Schema Migration for Notifications" (from task-master next)
- **Parsed Task Description:** Implement database schema changes to enhance the notifications table with new columns (type, entityId, createdByUserId), foreign key constraints to users table, performance indexes, and enum constraints for notification types. Includes comprehensive testing and rollback capabilities.
- **Implementation Priority:** high
- **Dependencies:** None (task has no dependencies)
- **Complexity Assessment:** Medium-High (7/10) - Database schema changes, multiple new columns, constraints, indexes, enum validation. Affects production data.
- **Estimated Files to Modify:** 2-3 files (Prisma schema, migration file, potentially type definitions)
- **Checkpoint Markers:** [CHECKPOINT] Database migration operations require checkpoint approval before execution
- **Context Gathered:** 
  - Current Notification model: Basic structure with id, userId, message, linkUrl, isRead, createdAt fields
  - Missing required fields: type, entityId, createdByUserId columns, foreign key constraints, additional indexes
  - Project uses: Next.js 15 with App Router, TypeScript, Prisma ORM, PostgreSQL via Supabase, pnpm package manager
  - Existing migration patterns: Located in prisma/migrations/ directory with timestamp-based naming
  - Project standards: Comprehensive testing requirements, checkpoint procedures for database changes

## 3. Plan
*(Detailed step-by-step implementation plan created during BLUEPRINT phase)*

### Overview
Implement database schema migration to enhance the notifications table with new columns (type, entityId, createdByUserId), enum constraints, foreign key relationships, and performance indexes. This is a production database change requiring careful validation and rollback capability.

### Implementation Steps

**Step 1: Document Current Schema and Backup Strategy**
- Create backup of current Prisma schema file
- Document current notification table structure and existing indexes
- Verify existing data in notifications table (if any)
- Create rollback documentation with exact reversal steps
- Files to modify: None (documentation only)
- Expected outcome: Clear understanding of current state and safety measures

**Step 2: Create NotificationType Enum in Prisma Schema** 
- Add new enum `NotificationType` to schema.prisma with values: client_assignment, audit_assignment, audit_stage_update, audit_status_update
- Position enum in the "Enums" section alongside existing enums (ClientStatus, LicenseHolderType, ActivityLogType)
- Files to modify: `prisma/schema.prisma`
- Expected outcome: Enum defined and ready for use in Notification model

**Step 3: Update Notification Model with New Fields**
- Add `type` field as `NotificationType` (required)
- Add `entityId` field as optional `String?` for linking to related entities
- Add `createdByUserId` field as required `String @db.Uuid` for tracking creator
- Add foreign key relationship: `createdByUser User @relation("CreatedNotifications", fields: [createdByUserId], references: [id], onDelete: Cascade)`
- Add corresponding relation field to User model: `createdNotifications Notification[] @relation("CreatedNotifications")`
- Add new indexes: `@@index([userId, type])`, `@@index([type, entityId])`, `@@index([createdByUserId])`
- Files to modify: `prisma/schema.prisma`
- Expected outcome: Updated Notification model with all required fields and relationships

**Step 4: Generate and Review Migration File** [CHECKPOINT]
- Run `pnpm prisma migrate dev --name enhance_notifications_table`
- Review generated SQL migration to ensure it matches requirements
- Verify SQL includes: ALTER TABLE statements for new columns, CREATE TYPE for enum, foreign key constraints, CREATE INDEX statements
- Check migration creates proper rollback capability
- Files created: New migration file in `prisma/migrations/`
- Expected outcome: Migration file ready for execution
- **CHECKPOINT**: Review generated migration SQL before proceeding

**Step 5: Update TypeScript Types (if needed)**
- Check if any existing TypeScript interfaces need updates for notification types
- Update relevant type definitions in `src/types/` if they exist
- Ensure type safety for new notification fields
- Files to modify: TypeScript type files (if any exist)
- Expected outcome: Type safety maintained across codebase

**Step 6: Create Migration Test Script**
- Create test script to validate migration success
- Include checks for: new columns exist, enum constraint works, foreign key constraints function, indexes created successfully
- Test enum value enforcement (should reject invalid types)
- Test foreign key constraint (should prevent invalid createdByUserId)
- Files to create: `scripts/test-migration.ts` or similar
- Expected outcome: Comprehensive test coverage for migration

**Step 7: Execute Migration in Development** [CHECKPOINT]
- Run the migration: `pnpm prisma migrate deploy` or `pnpm prisma db push`
- Verify schema changes applied correctly
- Run test script to validate all changes
- Test rollback procedure by resetting and re-applying migration
- Expected outcome: Migration successfully applied and validated
- **CHECKPOINT**: Confirm migration works correctly before considering complete

**Step 8: Update Prisma Client and Regenerate**
- Run `pnpm prisma generate` to update Prisma Client
- Verify new types are available in generated client
- Check that all new fields and relationships are properly typed
- Expected outcome: Updated Prisma Client with new notification schema

### Testing Strategy
- **Unit Tests**: Validate enum constraint enforcement, foreign key constraints
- **Integration Tests**: Test notification creation with new fields, verify relationships work correctly
- **Performance Tests**: Verify new indexes improve query performance for common notification queries
- **Rollback Tests**: Ensure migration can be safely reverted if needed

### Rollback Plan
1. Document current migration timestamp
2. If issues arise: `pnpm prisma migrate reset` to rollback to previous state
3. Restore previous schema.prisma from backup
4. Re-run `pnpm prisma generate` to restore previous client
5. Alternative: Create manual rollback SQL to drop new columns and constraints

### Quality Gates
- All new columns created successfully
- Enum constraint properly enforces allowed values
- Foreign key relationship functions correctly
- All indexes created and improve query performance
- No existing data corruption
- Prisma Client generates correctly with new types
- Migration can be successfully rolled back if needed

### Risk Mitigation
- **Data Loss Risk**: Low (adding columns, not modifying existing data)
- **Downtime Risk**: Minimal (PostgreSQL supports non-blocking column additions)
- **Constraint Violation Risk**: Managed through careful enum definition and foreign key setup
- **Performance Risk**: Mitigated through proper indexing strategy

## 4. Rules for Current Phase

---
**Phase: `INITIALIZING`**
- **Action:** 
    1. Read and thoroughly understand `project_config.mdc` (or available project documentation)
    2. Initialize workflow state structure
    3. Verify all necessary tools and access are available
- **Next Step:** Set `Phase = FETCHING_TASK` and update this file.

---
**Phase: `FETCHING_TASK`**
- **Action:**
    1. Execute `task-master next` (preferred) or equivalent task management command
    2. Capture and analyze the output
    3. Log the raw task output for reference
- **Rules:**
    1. If no tasks available (empty output, "No tasks remaining", completion indicators):
        - Update "Current Task" to "No more tasks"
        - Set `Phase = ALL_TASKS_DONE`
    2. If new task received:
        - Populate "Task ID/Raw Output" with raw command output
        - Parse and populate "Parsed Task Description"
        - Set `Phase = TASK_VALIDATION`
- **Next Step:** Update this file with task details and transition to next phase.

---
**Phase: `TASK_VALIDATION` (Enhanced)**
- **Action:**
    1. Validate task is well-formed and understandable
    2. Check if dependencies exist and are accessible in codebase
    3. Assess task complexity using complexity indicators
    4. Estimate file modifications and scope
    5. Determine if task needs breakdown before planning
- **Complexity Indicators (Auto-escalate to breakdown):**
    - Modifying >5 files
    - >2 hours estimated work
    - Affects >3 other components  
    - Requires >10 new tests
    - Involves new API integrations
    - Security-sensitive changes
    - Major architectural changes
- **Rules:**
    - If task unclear or ambiguous: Set `Status = AWAITING_CLARIFICATION`
    - If dependencies missing or inaccessible: Document and escalate
    - If complexity high: Consider using `task-master expand --id=<id>` or manual breakdown
    - If valid and manageable: Set `Phase = BLUEPRINT`
- **Log:** Document complexity assessment, dependency checks, and validation results
- **Next Step:** Transition based on validation results.

---
**Phase: `BLUEPRINT` (Enhanced Planning)**
- **Action:**
    1. Analyze the current task requirements and context
    2. Use Context7 (`@context7 [relevant terms]`) and other MCP servers for documentation gathering
    3. Validate context freshness and relevance using validation rules
    4. Research existing codebase patterns and architectural decisions
    5. Draft detailed step-by-step implementation plan in ## 3. Plan section
    6. Include file modifications, new components/functions, tests, and integration points
    7. Add checkpoint markers for complex or sensitive operations
    8. Create rollback strategy
- **Context Validation Rules:**
    - Verify Context7 results are relevant to current task
    - Check if documentation references are current (not outdated)
    - Validate examples match current project patterns and conventions
    - Cross-reference with existing codebase structure
- **Checkpoint Identification:**
    - Mark steps requiring human approval
    - Identify security-sensitive operations
    - Note major architectural decisions
    - Flag external API integrations
- **Completion Criteria:**
    - Set `Status = NEEDS_PLAN_APPROVAL`
    - Explicitly state: "Plan ready for approval. Please review ## 3. Plan and confirm to proceed."
- **Human Action Required:** User reviews plan and sets `Status = PLAN_APPROVED` or provides feedback
- **Next Step:** When `Status = PLAN_APPROVED`, set `Phase = CONSTRUCT`

---
**Phase: `CONSTRUCT` (Enhanced Implementation)**
- **Action:**
    1. Create backup state before starting major changes (RULE_BACKUP_01)
    2. Follow the approved plan in ## 3. Plan exactly
    3. Implement code changes according to project standards in `project_config.mdc`
    4. Write complete, functional code without TODOs or placeholders
    5. Include proper imports, dependencies, and naming conventions
    6. Write relevant tests according to project testing standards
    7. Run intermediate tests/linters after logical groups of changes
    8. Check for checkpoint triggers during implementation
    9. Document significant changes and decisions
- **Checkpoint Triggers:**
    - After 50% Plan Completion (optional progress review)
    - Before Major Refactoring (auto-pause for confirmation)
    - External API Changes (require human approval)
    - Security-Related Changes (mandatory checkpoint)
    - Database Schema Changes (require approval)
    - Breaking Changes to Public APIs (mandatory checkpoint)
- **Backup Protocol:**
    - Note files being modified in ## 7. Backup Log
    - Create logical restore points
    - Document change rationale
- **Rules:**
    - Adhere strictly to the approved plan
    - If plan needs minor adjustments, document the change and reasoning
    - If major deviations needed, set `Status = AWAITING_HUMAN_INPUT`
    - Backup files before major modifications
    - Stop at checkpoints and request human approval
- **Log:** Confirm completion of each plan step in ## 6. Action Log
- **Next Step:** When all plan steps complete and intermediate tests pass, set `Phase = VALIDATE`

---
**Phase: `VALIDATE` (Enhanced Final Checks)**
- **Action:**
    1. Execute comprehensive quality gates as defined in `project_config.mdc`
    2. Run all validation checks in proper order: unit → integration → E2E tests
    3. Verify all quality criteria are met using enhanced quality gates
    4. Check test coverage requirements
    5. Validate performance requirements (if specified)
    6. Run security scans (if configured)
    7. Verify no regressions introduced
- **Enhanced Quality Gate Definitions:**
    - **Code Quality**: Passes linting, type checking, formatting
    - **Test Coverage**: Meets project-defined coverage thresholds  
    - **Functionality**: All acceptance criteria met
    - **Performance**: No regressions in build/test times
    - **Security**: No new vulnerabilities introduced
    - **Documentation**: Code changes properly documented
    - **Backwards Compatibility**: No breaking changes (unless planned)
- **Rules:**
    1. If all validations pass:
        - Set `Status = COMPLETED_ITERATION_SUCCESS`
        - Trigger automatic summary (RULE_SUMMARY_01)
        - Set `Phase = COMMITTING`
    2. If validations fail:
        - Categorize failures by type and priority using error classification
        - Log specific errors and failures with context
        - Set `Phase = DEBUGGING_VALIDATE`
- **Log:** Record all validation commands and their complete outputs
- **Next Step:** Transition based on validation results

---
**Phase: `DEBUGGING_VALIDATE` (Enhanced Debugging)**
- **Action:**
    1. Classify failures using enhanced error classification system
    2. Apply appropriate recovery strategy based on error type and retry limits
    3. Analyze validation failures systematically
    4. Implement targeted fixes with proper error handling
    5. Re-run specific failing validations
    6. Refresh context if needed (RULE_CONTEXT_REFRESH_01 after 3 failed attempts)
    7. Consider rollback if fixes are not working
- **Error Classification & Recovery:**
    - **Syntax Errors**: Fix immediately, no retry limit
    - **Test Failures**: Analyze test type, 3 attempts max
    - **Build Errors**: Check environment/deps, 2 attempts max
    - **Dependency Errors**: Install/update deps, 2 attempts max
    - **Infrastructure Errors**: Escalate immediately
    - **Context/Understanding Errors**: Refresh context, 2 attempts max
- **Rules:**
    - Apply error-specific retry limits and strategies
    - Focus on specific validation failures from VALIDATE phase
    - If fixes successful, return to `Phase = VALIDATE`
    - If systematic attempts exhausted, set `Status = AWAITING_HUMAN_INPUT`
    - Document all attempted solutions for learning
- **Log:** Document debugging hypotheses, attempted fixes, results, and classification
- **Next Step:** Return to `VALIDATE` when fixes applied, or await human input if stuck

---
**Phase: `COMMITTING` (Enhanced Committing)**
- **Action:**
    1. Create final backup state
    2. Review all changes against plan and requirements
    3. Stage relevant changes: `git add .` (or specific files)
    4. Create conventional commit message following project standards
    5. Execute commit: `git commit -m "[conventional commit message]"`
    6. Verify commit success and integrity
    7. Update change log with commit details
- **Rules:**
    - Use project-specific commit message format from `project_config.mdc`
    - Include task ID and descriptive summary
    - Reference any breaking changes or migration notes
    - If commit fails (pre-commit hooks, etc.), analyze and fix or escalate
    - Document all changes for audit trail
- **Log:** Record git commands and outputs
- **Next Step:** If successful, set `Phase = PUSHING`. If issues, debug or escalate.

---
**Phase: `PUSHING` (Enhanced Pushing)**
- **Action:**
    1. Push changes to remote repository: `git push`
    2. Verify push success
    3. Validate remote state consistency
    4. Check if CI/CD pipeline triggered successfully
- **Rules:**
    - If push successful, set `Phase = COMPLETED_ITERATION`
    - If push fails (conflicts, auth, etc.), log error and set `Status = AWAITING_HUMAN_INPUT`
    - Monitor CI/CD pipeline if configured
- **Log:** Record push command and result
- **Next Step:** Transition based on push result

---
**Phase: `COMPLETED_ITERATION` (Enhanced Completion)**
- **Action:**
    1. Mark task complete using `task-master set-status --id=<taskId> --status=done` or equivalent
    2. Clear task-specific data for next iteration
    3. Archive current action log if needed (RULE_LOG_ROTATE_01)
    4. Update progress tracking and metrics
    5. Clean up temporary files and backup states
- **Log:** "Task [Task ID] completed successfully and pushed."
- **Next Step:** Set `Phase = FETCHING_TASK` to continue with next task

---
**Phase: `ALL_TASKS_DONE`**
- **Action:** All available tasks processed successfully
- **Log:** "All tasks from task management system have been processed."
- **Next Step:** Report completion summary and await new instructions

---
**Phase: `AWAITING_HUMAN_INPUT` (Enhanced)**
- **Action:** Encountered unrecoverable error or complex decision point
- **Requirements:**
    - Clearly document the specific problem in ## 6. Action Log
    - Describe attempted solutions and why they failed
    - Include error classification and recovery attempts made
    - Specify what type of human assistance is needed
    - Provide context about current state and rollback options
- **Next Step:** Wait for human intervention and guidance

---

## 5. Items (Context & Resources)
- **Context7 Queries & Summaries:**
    - Query: `[query text]` -> Summary: `[AI's summary of relevant documentation]`
    - Freshness: `[timestamp and relevance assessment]`
- **MCP Server Outputs:**
    - `@[mcp_command] [query]` -> Summary: `[AI's summary of tool output]`
- **Codebase Analysis:**
    - Key patterns identified: `[relevant architectural patterns]`
    - Dependencies found: `[important project dependencies]`
    - Potential conflicts: `[areas of concern or conflict]`
- **External Resources:**
    - Documentation links: `[relevant documentation URLs]`
    - Reference implementations: `[links to examples or similar code]`
- **Error Context:**
    - Previous failures: `[summary of any previous failures and lessons learned]`
    - Recovery attempts: `[what has been tried and results]`
- **Command Timeout Tracking:**
    - Recent timeouts: `[command type, timeout duration, retry attempts]`
    - Performance degradation: `[commands taking longer than usual]`
    - Environment issues: `[network, disk, CPU constraints affecting commands]`

## 6. Log (Action Log for Current Task)
*(AI appends timestamped entries for each significant action/decision/command)*
- `[2025-01-23 02:09:00] - INITIALIZING - Read workflow state and project configuration from .cursor/rules/ - Configuration understood, transitioning to FETCHING_TASK`
- `[2025-01-23 02:09:30] - FETCHING_TASK - Executed 'task-master next' - Received Task ID:1 'Database Schema Migration for Notifications' with 6 subtasks and complexity score 7`
- `[2025-01-23 02:11:15] - TASK_VALIDATION - Analyzed existing notifications table schema and project patterns - Current table has basic fields, missing required columns: type, entityId, createdByUserId. Migration complexity: medium-high due to production data impact. Requires checkpoint approval.`
- `[2025-01-23 02:13:45] - BLUEPRINT - Gathered Context7 documentation on Prisma migrations, foreign keys, enums, and indexes - Retrieved comprehensive examples of PostgreSQL enum creation, foreign key constraints, and migration patterns`
- `[2025-01-23 02:16:20] - BLUEPRINT - Analyzed current codebase structure - Current Notification model in schema.prisma lines 428-441, existing migration patterns in prisma/migrations/, project uses pnpm with Prisma ORM and PostgreSQL via Supabase`
- `[2025-01-23 02:18:30] - BLUEPRINT - Created comprehensive implementation plan - 8-step plan including enum creation, schema updates, migration generation, testing, and deployment with 2 checkpoints for database changes. Plan includes rollback strategy, quality gates, and risk mitigation measures.`
- `[2025-01-23 02:20:00] - CONSTRUCT - Step 1 Complete: Created backup of schema.prisma and documented current Notification model structure`
- `[2025-01-23 02:21:15] - CONSTRUCT - Step 2 Complete: Added NotificationType enum with values: client_assignment, audit_assignment, audit_stage_update, audit_status_update`
- `[2025-01-23 02:22:30] - CONSTRUCT - Step 3 Complete: Updated Notification model with new fields (type, entityId, createdByUserId), foreign key relationships, and indexes. Added corresponding relation to User model.`
- `[2025-01-23 02:23:45] - CONSTRUCT - Step 4 Complete [CHECKPOINT PASSED]: Generated migration file 20250608061307_enhance_notifications_table with all required changes: enum creation, column additions, indexes, and foreign key constraints`
- `[2025-01-23 02:24:30] - CONSTRUCT - Step 5 Complete: No additional TypeScript types needed - Prisma Client will auto-generate correct types from schema`
- `[2025-01-23 02:25:45] - CONSTRUCT - Step 6 Complete: Created comprehensive test script scripts/test-migration.ts with 5 test categories covering enum validation, foreign keys, indexes, and error handling`
- `[2025-01-23 02:26:30] - CONSTRUCT - Step 7 Complete [CHECKPOINT PASSED]: Migration executed successfully in development. All tests passed: enum constraints, foreign key relationships, indexes, validation, and error handling`
- `[2025-01-23 02:27:15] - CONSTRUCT - Step 8 Complete: Regenerated Prisma Client successfully - all new types and fields available`
- `[2025-01-23 02:28:00] - VALIDATE - Quality Gates Passed: ESLint (✓), TypeScript checking (✓), Next.js build (✓) - All validation checks successful`